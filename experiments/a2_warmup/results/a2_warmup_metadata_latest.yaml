experiment_name: a2_warmup
run_id: a2_warmup_20250714_161313
run_timestamp: '2025-07-14T16:13:13.609145'
configs_used:
  datasets:
    datasets:
      mmlu:
        load_params:
          path: cais/mmlu
          name: all
        split: test
        columns:
          input1: question
          input2: choices
          reference: answer
        preprocessing:
          input1_prefix: 'Answer with a single letter only.

            Question: '
          input2_prefix: 'Options:

            '
          output_prefix: 'Answer: '
          reference_mapping: index_to_letter
        extraction:
          method: multiple_choice
          pattern: ([A-D])
        generation:
          max_new_tokens: 1
          stop_sequence:
          - '#'
      gsm8k:
        load_params:
          path: gsm8k
          name: main
        split: test
        columns:
          input1: question
          reference: answer
        preprocessing:
          input1_prefix: 'Solve this step by step: '
        extraction:
          method: regex
          pattern: (\d+)
        generation:
          max_new_tokens: 256
          stop_sequence:
          - '

            '
      cnn_dailymail:
        load_params:
          path: cnn_dailymail
          name: 3.0.0
        split: test
        columns:
          input1: article
          reference: highlights
        preprocessing:
          input1_prefix: 'Summarize: '
        extraction:
          method: raw
        generation:
          max_new_tokens: 256
          stop_sequence:
          - '###'
  models:
    phi2:
      name: microsoft/phi-2
      parameter_count: 2
      type: general
    phi3:
      name: microsoft/Phi-3-mini-4k-instruct
      parameter_count: 3
      type: general
    qwen2.5-7b:
      name: Qwen/Qwen2.5-7B
      parameter_count: 7
      type: general
    qwen2.5-14b:
      name: Qwen/Qwen2.5-14B-Instruct
      parameter_count: 14
      type: general
    qwen2.5-3b:
      name: Qwen/Qwen2.5-3B-Instruct
      parameter_count: 3
      type: general
    qwen2.5-1.5b:
      name: Qwen/Qwen2.5-1.5B-Instruct
      parameter_count: 1.5
      type: general
    qwen2.5-0.5b:
      name: Qwen/Qwen2.5-0.5B-Instruct
      parameter_count: 0.5
      type: general
    mistral-7b:
      name: mistralai/Mistral-7B-Instruct-v0.3
      parameter_count: 7
      type: general
    gemma-3-27b:
      name: google/gemma-3-27b-it
      parameter_count: 27
      type: general
    gemma-3-12b:
      name: google/gemma-3-12b-it
      parameter_count: 12
      type: general
    gemma-3-4b:
      name: google/gemma-3-4b-it
      parameter_count: 4
      type: general
    gemma-3-1b:
      name: google/gemma-3-1b-it
      parameter_count: 1
      type: general
    phi-4-mini:
      name: microsoft/Phi-4-mini-instruct
      parameter_count: 4
      type: general
    phi-4:
      name: microsoft/phi-4
      parameter_count: 15
      type: general
    llama-3.1-8b:
      name: meta-llama/Llama-3.1-8B-Instruct
      parameter_count: 8
      type: general
    llama-3.2-3b:
      name: meta-llama/Llama-3.2-3B-Instruct
      parameter_count: 3
      type: general
    llama-3.2-1b:
      name: meta-llama/Llama-3.2-1B-Instruct
      parameter_count: 1
      type: general
    Yi-34B:
      name: 01-ai/Yi-34B
      parameter_count: 34
      type: general
    aya-expanse-32b:
      name: CohereForAI/aya-expanse-32b
      parameter_count: 32
      type: general
  mab:
    epsilon_greedy:
      initial_epsilon: 0.1
      decay_factor: 0.995
      min_epsilon: 0.01
    contextual_epsilon_greedy:
      initial_epsilon: 0.1
      decay_factor: 0.995
      min_epsilon: 0.01
    linear_epsilon_greedy:
      initial_epsilon: 0.5
      decay_factor: 0.995
      min_epsilon: 0.01
      lambda_: 0.1
    linucb:
      alpha: 0.1
      regularization: 0.1
    thompson_sampling:
      sigma: 0.1
      prior_variance: 2.0
  experiments:
    defaults:
      datasets:
      - all
      samples_per_dataset: 500
      model_ids:
      - all
      random_seed: 40
      lambda_weight: 0.5
      n_runs: 20
      features: &id003
      - dataset_index
      - complexity
      - semantic_cluster
    plotting_defaults:
      dpi: 300
      formats: &id001
      - png
      - pdf
      fonts: &id002
        base_size: 10
        label_size: 11
        tick_size: 9
        title_size: 13
        annotation_size: 8
      grid_alpha: 0.6
      confidence_level: 0.95
    epsilon_greedy_defaults:
      initial_epsilon: 1.0
      decay_factor: 0.98
      min_epsilon: 0.01
    linear_epsilon_greedy_defaults:
      initial_epsilon: 1.0
      decay_factor: 0.985
      min_epsilon: 0.01
      lambda_: 0.25
    linucb_defaults:
      alpha: 0.1
      regularization: 0.05
    thompson_sampling_defaults:
      sigma: 0.25
      prior_variance: 2
    a1_static_sanity_check:
      category: Static Baseline Evaluation
      datasets:
      - all
      samples_per_dataset: 500
      model_ids:
      - all
      n_runs: 1
      random_seed: 40
      plotting:
        dpi: 300
        formats:
        - png
        - pdf
        grid_alpha: 0.6
        envelope_plot:
          fonts:
            annotation_size: 6
            baseline_annotation_size: 8
            label_size: 9
            tick_size: 8
            legend_size: 9
          marker_sizes:
            models: 15
            baselines: 75
            pareto_points: 3
        cumulative_energy_plot:
          fonts:
            annotation_size: 7
            baseline_annotation_size: 9
            label_size: 11
            tick_size: 9
            legend_size: 9
          marker_sizes:
            models: 20
            baselines: 120
            pareto_points: 3
    a2_warmup:
      category: Algorithm Warm-up
      lambda_weight: 0.5
      random_seed: 40
      algorithms:
        epsilon_greedy:
          initial_epsilon: 1.0
          decay_factor: 0.98
          min_epsilon: 0.01
        contextual_epsilon_greedy:
          initial_epsilon: 1.0
          decay_factor: 0.985
          min_epsilon: 0.01
          lambda_: 0.25
        linucb:
          alpha: 0.1
          regularization: 0.05
      plotting:
        dpi: 300
        formats:
        - png
        - pdf
        regret_plot:
          fonts:
            base_size: 10
            label_size: 9
            tick_size: 8
            legend_size: 8
            title_size: 12
          line_width: 2
          confidence_alpha: 0.2
          show_confidence_intervals: true
        heatmap:
          colormap: Blues
          annotation_format: .3f
    a3_feature_ablation:
      category: Feature Ablation
      active_algorithm: linucb
      lambda_weight: 0.5
      random_seed: 40
      n_runs: 20
      feature_sets:
      - none
      - task
      - cluster
      - complex
      - task_cluster
      - task_complex
      - cluster_complex
      - full
      exclude_models: []
      algorithms:
        epsilon_greedy:
          initial_epsilon: 1.0
          decay_factor: 0.98
          min_epsilon: 0.01
        linear_epsilon_greedy:
          initial_epsilon: 1.0
          decay_factor: 0.985
          min_epsilon: 0.01
          lambda_: 0.25
        linucb:
          alpha: 0.1
          regularization: 0.05
        thompson_sampling:
          sigma: 0.25
          prior_variance: 2
      plotting:
        dpi: 300
        formats: *id001
        fonts: *id002
        grid_alpha: 0.6
        confidence_level: 0.95
        bar_plot:
          alpha: 0.8
          label_format: '%.3f'
          padding: 3
        box_plot:
          show_mean: false
    a4_hyperparameter_tuning:
      category: Hyperparameter Tuning for MABs
      n_runs: 10
      lambda_weight: 0.5
      random_seed: 40
      features: *id003
      algorithms:
        epsilon_greedy:
          initial_epsilon: 1.0
          decay_factor:
          - 0.95
          - 0.98
          - 0.99
          min_epsilon:
          - 0.005
          - 0.01
          - 0.02
        linear_epsilon_greedy:
          initial_epsilon: 1.0
          decay_factor:
          - 0.98
          - 0.99
          - 0.995
          min_epsilon:
          - 0.01
          - 0.02
          - 0.03
          lambda_:
          - 0.1
          - 0.25
          - 0.5
          - 1.0
        linucb:
          alpha:
          - 0.05
          - 0.1
          - 0.25
          regularization:
          - 0.005
          - 0.01
          - 0.02
          - 0.05
        thompson_sampling:
          sigma:
          - 0.1
          - 0.2
          prior_variance:
          - 1.0
          - 2.0
          - 3.0
          - 5.0
      plotting:
        dpi: 300
        formats: *id001
        fonts: *id002
        grid_alpha: 0.6
        confidence_level: 0.95
        figsize:
        - 12
        - 6
        timeline:
          figsize_base:
          - 18
          - 6
          marker_size: 5
          alpha: 0.1
    a5_algorithm_bakeoff:
      category: Algorithm Bakeoff
      n_runs: 20
      lambda_weight: 0.5
      random_seed: 40
      features: *id003
      algorithms:
        epsilon_greedy:
          initial_epsilon: 1.0
          decay_factor: 0.98
          min_epsilon: 0.01
        linear_epsilon_greedy:
          initial_epsilon: 1.0
          decay_factor: 0.985
          min_epsilon: 0.01
          lambda_: 0.25
        linucb:
          alpha: 0.1
          regularization: 0.05
        thompson_sampling:
          sigma: 0.25
          prior_variance: 2
      plotting:
        dpi: 300
        formats: *id001
        fonts:
          base_size: 10
          label_size: 12
          tick_size: 9
          title_size: 13
          annotation_size: 8
        grid_alpha: 0.6
        confidence_level: 0.95
        figsize:
        - 10
        - 7
        marker_sizes:
          models: 60
          baselines: 120
          algorithms: 100
          pareto_line: 3
    a6_lambda_sweep:
      category: Lambda Parameter Sweep
      n_runs: 20
      random_seed: 40
      lambda_weight: 0.5
      features: *id003
      lambda_values:
      - 0.0
      - 0.1
      - 0.2
      - 0.3
      - 0.4
      - 0.5
      - 0.6
      - 0.7
      - 0.8
      - 0.9
      - 1.0
      algorithms:
        epsilon_greedy:
          initial_epsilon: 1.0
          decay_factor: 0.98
          min_epsilon: 0.01
        linear_epsilon_greedy:
          initial_epsilon: 1.0
          decay_factor: 0.985
          min_epsilon: 0.01
          lambda_: 0.25
        linucb:
          alpha: 0.1
          regularization: 0.05
        thompson_sampling:
          sigma: 0.25
          prior_variance: 2
      plotting:
        dpi: 300
        formats: *id001
        fonts: *id002
        grid_alpha: 0.6
        confidence_level: 0.95
        figsize:
        - 10
        - 7
    a8_adaptability:
      category: Model Pool Adaptability
      n_runs: 20
      lambda_weight: 0.5
      random_seed: 40
      features: *id003
      change_point_query_index: 1000
      model_to_add: gemma-3-12b
      model_to_remove: llama-3.1-8b
      active_algorithm: linucb
      algorithms:
        linucb:
          alpha: 0.1
          regularization: 0.05
        thompson_sampling:
          sigma: 0.25
          prior_variance: 2
      plotting:
        dpi: 300
        formats: *id001
        fonts: *id002
        grid_alpha: 0.6
        confidence_level: 0.95
        cumulative_regret:
          figsize:
          - 12
          - 7
          show_confidence_intervals: true
        model_selection:
          figsize:
          - 15
          - 8
          bin_size: 25
        timeline:
          figsize_base:
          - 18
          - 6
          marker_size: 5
algorithms_compared:
- random
- epsilon_greedy
n_runs: 5
base_random_seed: 40
